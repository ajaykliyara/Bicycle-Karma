{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Pull id's of all bike data available on BikeIndex.Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#package for JSON interaction\n",
    "import json\n",
    "#package to interact with URL's\n",
    "import urllib2\n",
    "\n",
    "#time to sleep\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes a bikeindex api url and page number and extracts all required attirbutes from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to pull bike index data\n",
    "def bike_parser(url,page_no):\n",
    "    time.sleep(0.5)\n",
    "    #open url\n",
    "    f = urllib2.urlopen(url)\n",
    "\n",
    "    #load json from URL, dict returned\n",
    "    data = json.load(f)\n",
    "    \n",
    "    data_bikes = data['bikes']\n",
    "    bike_df = pd.DataFrame(data_bikes,columns=['id','serial','title','manufacturer_name','frame_model','year','frame_colors','is_stock_img','large_img','thumb','stolen','date_stolen','stolen_location'])\n",
    "    bike_df['url'] = url.encode('utf-8')\n",
    "    bike_df['page_no'] = page_no\n",
    "\n",
    "    return bike_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating blank dataframe to house bike index data that is going to be pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike_df = pd.DataFrame(columns=['id','serial','title','manufacturer_name','frame_model','year','frame_colors','is_stock_img','large_img','thumb','stolen','date_stolen','stolen_location','url','page_no'])\n",
    "cont_flag = True\n",
    "page_no = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next loop through all pages in bikeindex and hold the data in a csv file. This process take about an hour to complete. Also every now and then the api tends to go down and we will have to resume the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages parsed = 10, Records parsed = 800\n",
      "Pages parsed = 20, Records parsed = 1600\n",
      "Pages parsed = 30, Records parsed = 2400\n",
      "Pages parsed = 40, Records parsed = 3200\n",
      "Pages parsed = 50, Records parsed = 4000\n",
      "Pages parsed = 60, Records parsed = 4800\n",
      "Pages parsed = 70, Records parsed = 5600\n",
      "Pages parsed = 80, Records parsed = 6400\n",
      "Pages parsed = 90, Records parsed = 7200\n",
      "Pages parsed = 100, Records parsed = 8000\n",
      "Pages parsed = 110, Records parsed = 8800\n",
      "Pages parsed = 120, Records parsed = 9600\n",
      "Pages parsed = 130, Records parsed = 10400\n",
      "Pages parsed = 140, Records parsed = 11200\n",
      "Pages parsed = 150, Records parsed = 12000\n",
      "Pages parsed = 160, Records parsed = 12800\n",
      "Pages parsed = 170, Records parsed = 13600\n",
      "Pages parsed = 180, Records parsed = 14400\n",
      "Pages parsed = 190, Records parsed = 15200\n",
      "Pages parsed = 200, Records parsed = 16000\n",
      "Pages parsed = 210, Records parsed = 16800\n",
      "Pages parsed = 220, Records parsed = 17600\n",
      "Pages parsed = 230, Records parsed = 18400\n",
      "Pages parsed = 240, Records parsed = 19200\n",
      "Pages parsed = 250, Records parsed = 20000\n",
      "Pages parsed = 260, Records parsed = 20800\n",
      "Pages parsed = 270, Records parsed = 21600\n",
      "Pages parsed = 280, Records parsed = 22400\n",
      "Pages parsed = 290, Records parsed = 23200\n",
      "Pages parsed = 300, Records parsed = 24000\n",
      "Pages parsed = 310, Records parsed = 24800\n",
      "Pages parsed = 320, Records parsed = 25600\n",
      "Pages parsed = 330, Records parsed = 26400\n",
      "Pages parsed = 340, Records parsed = 27200\n",
      "Pages parsed = 350, Records parsed = 28000\n",
      "Pages parsed = 360, Records parsed = 28800\n",
      "Pages parsed = 370, Records parsed = 29600\n",
      "Pages parsed = 380, Records parsed = 30400\n",
      "Pages parsed = 390, Records parsed = 31200\n",
      "Pages parsed = 400, Records parsed = 32000\n",
      "Pages parsed = 410, Records parsed = 32800\n",
      "Pages parsed = 420, Records parsed = 33600\n",
      "Pages parsed = 430, Records parsed = 34400\n",
      "Pages parsed = 440, Records parsed = 35200\n",
      "Pages parsed = 450, Records parsed = 36000\n",
      "Pages parsed = 460, Records parsed = 36800\n",
      "Pages parsed = 470, Records parsed = 37600\n",
      "Pages parsed = 480, Records parsed = 38400\n",
      "Pages parsed = 490, Records parsed = 39200\n",
      "Pages parsed = 500, Records parsed = 40000\n",
      "Pages parsed = 510, Records parsed = 40800\n",
      "Pages parsed = 520, Records parsed = 41600\n",
      "Pages parsed = 530, Records parsed = 42400\n",
      "Pages parsed = 540, Records parsed = 43200\n",
      "Pages parsed = 550, Records parsed = 44000\n",
      "Pages parsed = 560, Records parsed = 44800\n",
      "Pages parsed = 570, Records parsed = 45600\n",
      "Pages parsed = 580, Records parsed = 46400\n",
      "Pages parsed = 590, Records parsed = 47200\n",
      "Pages parsed = 600, Records parsed = 48000\n",
      "Pages parsed = 610, Records parsed = 48800\n",
      "Pages parsed = 620, Records parsed = 49600\n",
      "Pages parsed = 630, Records parsed = 50400\n",
      "Pages parsed = 640, Records parsed = 51200\n",
      "Pages parsed = 650, Records parsed = 52000\n",
      "Pages parsed = 660, Records parsed = 52800\n",
      "Pages parsed = 670, Records parsed = 53600\n",
      "Pages parsed = 680, Records parsed = 54400\n",
      "Pages parsed = 690, Records parsed = 55200\n",
      "Pages parsed = 700, Records parsed = 56000\n",
      "Pages parsed = 710, Records parsed = 56800\n",
      "Pages parsed = 720, Records parsed = 57600\n",
      "Pages parsed = 730, Records parsed = 58400\n",
      "Pages parsed = 740, Records parsed = 59200\n",
      "Reached end of Pages, # of pages read = 748, # of records read = 59729\n"
     ]
    }
   ],
   "source": [
    "#Looping through all pages in bikeindex, extract records one page at a time\n",
    "while(cont_flag):\n",
    "    url = 'https://bikeindex.org:443/api/v2/bikes_search?page=' + str(page_no) + '&per_page=80'\n",
    "\n",
    "    #dataframe with 100 bikes that was read from url\n",
    "    temp_bike_df = bike_parser(url,page_no)\n",
    "    \n",
    "    #checking # of records in page\n",
    "    if (temp_bike_df.shape[0] == 0):\n",
    "        print 'Reached end of Pages, # of pages read = %d, # of records read = %d'%(page_no,bike_df.shape[0])\n",
    "        cont_flag = False\n",
    "        break\n",
    "    \n",
    "    with open('bike_csv.csv', 'a') as f:\n",
    "        if page_no == 1:\n",
    "            temp_bike_df.to_csv(f, header=True,encoding='utf8')\n",
    "        else:\n",
    "            temp_bike_df.to_csv(f, header=False,encoding='utf8')\n",
    "\n",
    "    #appending temp df to final df\n",
    "    bike_df = bike_df.append(temp_bike_df,ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    #inserting print to get an indication of number of pages parsed so far\n",
    "    if page_no%10 == 0:\n",
    "        print('Pages parsed = %d, Records parsed = %d'%(page_no,bike_df.shape[0]))\n",
    "    \n",
    "    page_no = page_no + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A check was put in place to ensure that number of records extracted was approximately same as what was in the header variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in Response header 59724,\n",
      " Records in final bike df 59723\n"
     ]
    }
   ],
   "source": [
    "#checking if total number of records are read\n",
    "\n",
    "#calling url to get response\n",
    "f = urllib2.urlopen('https://bikeindex.org:443/api/v2/bikes_search?page=1')\n",
    "\n",
    "#read total number of records from header\n",
    "Total_rec_response = f.info().getheader('Total')\n",
    "\n",
    "#Both must be equal\n",
    "print 'Records in Response header %d,\\n Records in final bike df %d'%(int(Total_rec_response),bike_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data had duplicates this is because API did not maintain the set number of records in a page. This set changed everytime a call was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51916"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bike_df.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike_df.to_csv(\"bike_df.csv\",encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To over come the duplication problem , ran through the above process for different number of records per page and finall combined all datasets eliminating duplicates. This method allowed for getting most of the available data on BikeIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#api does not follow the same order during searches, to address this pulled data 25,75,100,90 per page with the intention with multiple\n",
    "#pulls we will be able to get more id's\n",
    "bike1 = pd.read_csv('old_bike_csv.csv')\n",
    "bike2 = pd.read_csv('bike_csv_100.csv')\n",
    "bike3 = pd.read_csv('bike_csv_75.csv')\n",
    "bike4 = pd.read_csv('bike_csv_90.csv')\n",
    "bike5 = pd.read_csv('bike_csv_80.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id1 = bike1.id.unique().tolist()\n",
    "id2 = bike2.id.unique().tolist()\n",
    "id3 = bike3.id.unique().tolist()\n",
    "id4 = bike4.id.unique().tolist()\n",
    "id5 = bike5.id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id1 = [str(i) for i in id1]\n",
    "id2 = [str(i) for i in id2]\n",
    "id3 = [str(i) for i in id3]\n",
    "id4 = [str(i) for i in id4]\n",
    "id5 = [str(i) for i in id5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_f = id1\n",
    "id_f.extend(id2)\n",
    "id_f.extend(id3)\n",
    "id_f.extend(id4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Set of id's obtained from bikeindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59624"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking final list of records obtained.\n",
    "len(set(id_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of id's is written onto a file which will be used later to get all attributes pertaining to a specific id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_s = pd.Series(list(set(id_f)))\n",
    "id_s.name = 'id'\n",
    "id_s.to_csv(\"bike_id_master.csv\",header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
